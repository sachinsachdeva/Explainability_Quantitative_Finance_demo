{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cf8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dspy\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.stats import ks_2samp\n",
    "import shap\n",
    "\n",
    "# Optional: XGBoost can be used instead; this example uses sklearn's GB for simplicity.\n",
    "\n",
    "# ----- User settings -----\n",
    "OUT_DIR = Path(\"./explainable_ai_demo\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# use your favorite LLM provider (check DSPy documentation for details)\n",
    "GOOGLE_API_KEY = (\n",
    "    \"<gemini_api_key>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24950f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7260ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model — accuracy: 0.945, AUC: 0.980\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1) Data generation and model training\n",
    "# -----------------------------------------------------------\n",
    "rng = np.random.RandomState(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=3000,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    n_redundant=2,\n",
    "    n_clusters_per_class=2,\n",
    "    flip_y=0.02,\n",
    "    class_sep=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "feature_names = [f\"feat_{i}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df[\"approved\"] = y\n",
    "\n",
    "# synthetic sensitive attribute correlated with label to demonstrate bias checks\n",
    "gender = (rng.rand(len(df)) < (0.45 + 0.2 * (df[\"approved\"]))).astype(int)\n",
    "df[\"gender\"] = gender  # 1: Group A, 0: Group B\n",
    "\n",
    "# train/test split\n",
    "X_features = feature_names + [\"gender\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[X_features],\n",
    "    df[\"approved\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"approved\"],\n",
    ")\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# baseline metrics\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Trained model — accuracy: {acc:.3f}, AUC: {auc:.3f}\")\n",
    "\n",
    "# Save small model metadata for regulator view\n",
    "MODEL_METADATA = {\n",
    "    \"version\": \"0.1\",\n",
    "    \"trained_on\": str(datetime.date.today()),\n",
    "    \"n_train\": len(X_train),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5196c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2) Tools (to be exposed to DSPy / the LLM agent)\n",
    "#     We also create a very light tool-call logger to capture the transcript.\n",
    "# -----------------------------------------------------------\n",
    "TOOL_LOG = []  # append tuples (timestamp, tool_name, args, result_summary)\n",
    "\n",
    "\n",
    "def log_tool_call(tool_name, args, result):\n",
    "    TOOL_LOG.append(\n",
    "        {\n",
    "            \"time\": str(datetime.datetime.utcnow()),\n",
    "            \"tool\": tool_name,\n",
    "            \"args\": args if len(str(args)) < 2000 else str(args)[:2000],\n",
    "            \"result_summary\": (result if isinstance(result, dict) else str(result))\n",
    "            if len(str(result)) < 2000\n",
    "            else str(result)[:2000],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Tool: predict_customer\n",
    "def predict_customer(customer_row):\n",
    "    \"\"\"Run model prediction on a single customer (Series or dict).\"\"\"\n",
    "    # Ensure correct order of columns\n",
    "    if isinstance(customer_row, dict):\n",
    "        row = pd.DataFrame([customer_row])[X_features]\n",
    "    else:\n",
    "        row = (\n",
    "            pd.DataFrame([customer_row.values], columns=customer_row.index)[X_features]\n",
    "            if isinstance(customer_row, pd.Series)\n",
    "            else pd.DataFrame([customer_row], columns=X_features)\n",
    "        )\n",
    "    pred = int(model.predict(row)[0])\n",
    "    proba = float(model.predict_proba(row)[0][1])\n",
    "    result = {\"prediction\": pred, \"probability\": proba}\n",
    "    log_tool_call(\n",
    "        \"predict_customer\",\n",
    "        {\"row\": (row.to_dict(orient=\"records\")[0])},\n",
    "        {\"prediction\": pred, \"probability\": round(proba, 4)},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "# Tool: explain_prediction (use SHAP TreeExplainer for per-instance explanations)\n",
    "def explain_prediction(customer_row, top_k=5, save_plot=True, plot_path=None):\n",
    "    \"\"\"Return SHAP per-feature contributions and save a SHAP bar plot for regulator visuals.\"\"\"\n",
    "    # Prepare instance\n",
    "    if isinstance(customer_row, dict):\n",
    "        row = pd.DataFrame([customer_row])[X_features]\n",
    "    else:\n",
    "        row = (\n",
    "            pd.DataFrame([customer_row.values], columns=customer_row.index)[X_features]\n",
    "            if isinstance(customer_row, pd.Series)\n",
    "            else pd.DataFrame([customer_row], columns=X_features)\n",
    "        )\n",
    "    # SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(row)  # returns array shape (1, n_features)\n",
    "    # create mapping\n",
    "    contributions = dict(zip(X_features, shap_values[0].tolist()))\n",
    "    # top k by absolute magnitude\n",
    "    top = sorted(contributions.items(), key=lambda kv: -abs(kv[1]))[:top_k]\n",
    "    result = {\"feature_contributions\": top, \"raw\": contributions}\n",
    "    log_tool_call(\n",
    "        \"explain_prediction\",\n",
    "        {\"row\": row.to_dict(orient=\"records\")[0]},\n",
    "        {\"top_features\": top},\n",
    "    )\n",
    "    # optionally save a SHAP bar plot for regulator\n",
    "    if save_plot:\n",
    "        if plot_path is None:\n",
    "            plot_path = OUT_DIR / f\"shap_{str(np.random.randint(0, 999999))}.png\"\n",
    "        else:\n",
    "            plot_path = Path(plot_path)\n",
    "        # Plot a simple bar chart of contributions\n",
    "        feats = [f for f, _ in top]\n",
    "        vals = [v for _, v in top]\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        y_pos = np.arange(len(feats))\n",
    "        plt.barh(y_pos, vals[::-1])\n",
    "        plt.yticks(y_pos, feats[::-1])\n",
    "        plt.xlabel(\"SHAP contribution\")\n",
    "        plt.title(\"Top feature contributions (local SHAP)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path, dpi=150)\n",
    "        plt.close()\n",
    "        result[\"plot_path\"] = str(plot_path)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Tool: check_model_drift\n",
    "def check_model_drift(new_data):\n",
    "    \"\"\"KS-test per feature between training set and new_data (p < 0.05 -> flagged).\"\"\"\n",
    "    drift = {}\n",
    "    for col in X_train.columns:\n",
    "        stat, pval = ks_2samp(X_train[col], new_data[col])\n",
    "        drift[col] = {\"statistic\": float(stat), \"p_value\": float(pval)}\n",
    "    drifted = [c for c, v in drift.items() if v[\"p_value\"] < 0.05]\n",
    "    result = {\"drifted_features\": drifted, \"per_feature\": drift}\n",
    "    log_tool_call(\"check_model_drift\", {\"rows\": len(new_data)}, {\"drifted\": drifted})\n",
    "    return result\n",
    "\n",
    "\n",
    "# Tool: bias_audit (group rates and disparate impact)\n",
    "def bias_audit(dataset, sensitive_feature=\"gender\"):\n",
    "    \"\"\"Compute acceptance rates per group and disparate impact (group0/group1).\"\"\"\n",
    "    # Make sure dataset has required columns\n",
    "    ds = dataset.copy()\n",
    "    preds = model.predict(ds[X_features])\n",
    "    ds[\"pred\"] = preds\n",
    "    rates = ds.groupby(sensitive_feature)[\"pred\"].mean().to_dict()\n",
    "    g0 = rates.get(0, 0.0)\n",
    "    g1 = rates.get(1, 0.0)\n",
    "    di = (g0 / g1) if g1 > 0 else None\n",
    "    result = {\n",
    "        \"group_rates\": rates,\n",
    "        \"disparate_impact\": di,\n",
    "        \"counts\": ds[sensitive_feature].value_counts().to_dict(),\n",
    "    }\n",
    "    log_tool_call(\n",
    "        \"bias_audit\", {\"rows\": len(ds)}, {\"disparate_impact\": di, \"rates\": rates}\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41fa1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy + Gemini configured. Running real LLM agent (Gemini).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3) DSPy + Gemini agent (or simulated agent)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "import dspy\n",
    "\n",
    "# Configure Gemini LLM as DSPy backend\n",
    "model_name = \"gemini/gemini-2.5-flash\"\n",
    "lm = dspy.LM(\n",
    "    model_name,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "print(\"DSPy + Gemini configured. Running real LLM agent (Gemini).\")\n",
    "\n",
    "# Helper: formatting templates for roles\n",
    "def format_client(prediction_info, explanation):\n",
    "    pred_text = \"approved\" if prediction_info[\"prediction\"] == 1 else \"rejected\"\n",
    "    reasons = \", \".join(\n",
    "        [f\"{feat} ({val:.3f})\" for feat, val in explanation[\"feature_contributions\"]]\n",
    "    )\n",
    "    prob = prediction_info[\"probability\"]\n",
    "    suggestion = \"You could improve your likelihood by improving income or reducing debt. Contact the bank to discuss options.\"\n",
    "    return f\"Decision: Your application was {pred_text} (confidence {prob:.2f}). Main influences: {reasons}. {suggestion}\"\n",
    "\n",
    "\n",
    "def format_regulator(prediction_info, explanation, metadata):\n",
    "    lines = []\n",
    "    lines.append(f\"Model version: {metadata['version']}\")\n",
    "    lines.append(\n",
    "        f\"Model trained on: {metadata['trained_on']} (rows: {metadata['n_train']})\"\n",
    "    )\n",
    "    lines.append(\n",
    "        f\"Prediction: {prediction_info['prediction']}, probability: {prediction_info['probability']:.4f}\"\n",
    "    )\n",
    "    lines.append(\"Top local feature contributions (SHAP):\")\n",
    "    for feat, val in explanation[\"feature_contributions\"]:\n",
    "        lines.append(f\" - {feat}: {val:.6f}\")\n",
    "    lines.append(f\"Data lineage: features used = {X_features}\")\n",
    "    if \"plot_path\" in explanation:\n",
    "        lines.append(f\"SHAP plot: {explanation['plot_path']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def format_executive(drift_info, bias_info):\n",
    "    kpis = []\n",
    "    kpis.append(f\"Model AUC: {auc:.3f}\")\n",
    "    kpis.append(f\"Drifted features: {drift_info['drifted_features']}\")\n",
    "    kpis.append(f\"Acceptance rates: {bias_info['group_rates']}\")\n",
    "    kpis.append(f\"Disparate impact (group0/group1): {bias_info['disparate_impact']}\")\n",
    "    return \" | \".join(kpis)\n",
    "\n",
    "\n",
    "# DSPy program definition\n",
    "class StakeholderQuery(dspy.Signature):\n",
    "    role = dspy.InputField(desc=\"Stakeholder role: Client, Regulator, Executive\")\n",
    "    customer_data = dspy.InputField(desc=\"Customer features as JSON string\")\n",
    "    output = dspy.OutputField(desc=\"Formatted explanation\")\n",
    "\n",
    "\n",
    "# Wrap tools as DSPy Tools, with small wrapper functions to ensure serializable args\n",
    "def t_predict_customer(customer_json):\n",
    "    customer = json.loads(customer_json)\n",
    "    return predict_customer(customer)\n",
    "\n",
    "\n",
    "def t_explain_prediction(customer_json):\n",
    "    customer = json.loads(customer_json)\n",
    "    return explain_prediction(\n",
    "        customer, save_plot=True, plot_path=OUT_DIR / \"reg_shap.png\"\n",
    "    )\n",
    "\n",
    "\n",
    "def t_check_model_drift(json_window):\n",
    "    window = pd.DataFrame(json.loads(json_window))\n",
    "    return check_model_drift(window)\n",
    "\n",
    "\n",
    "def t_bias_audit(json_window):\n",
    "    window = pd.DataFrame(json.loads(json_window))\n",
    "    return bias_audit(window)\n",
    "\n",
    "\n",
    "predict_tool = dspy.Tool(\n",
    "    t_predict_customer, name=\"predict_customer\", desc=\"Predicts loan approval\"\n",
    ")\n",
    "explain_tool = dspy.Tool(\n",
    "    t_explain_prediction,\n",
    "    name=\"explain_prediction\",\n",
    "    desc=\"Explains the prediction with SHAP\",\n",
    ")\n",
    "drift_tool = dspy.Tool(\n",
    "    t_check_model_drift,\n",
    "    name=\"check_model_drift\",\n",
    "    desc=\"Checks model drift via KS tests\",\n",
    ")\n",
    "bias_tool = dspy.Tool(\n",
    "    t_bias_audit,\n",
    "    name=\"bias_audit\",\n",
    "    desc=\"Checks group acceptance rates and disparate impact\",\n",
    ")\n",
    "\n",
    "# Register tools (DSPy will expose them to Gemini)\n",
    "PROGRAM_TOOLS = [predict_tool, explain_tool, drift_tool, bias_tool]\n",
    "\n",
    "\n",
    "# Example DSPy Module where the LLM decides which tools to call\n",
    "class ExplainableAIAgent(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ReAct(StakeholderQuery, tools=PROGRAM_TOOLS)\n",
    "\n",
    "    def forward(self, role, customer_data=None):\n",
    "        # we pass strings so the LLM can directly receive the JSON\n",
    "        cust_json = json.dumps(customer_data) if customer_data is not None else \"\"\n",
    "        # ask LLM to generate an explanation; DSPy will allow tool-calls inside\n",
    "        response = self.predictor(role=role, customer_data=cust_json)\n",
    "        return response.output\n",
    "\n",
    "\n",
    "# instantiate agent\n",
    "dsp_agent = ExplainableAIAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fca8d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running demo runs ---\n",
      "\n",
      "=== Invoking DSPy + Gemini agent (real LLM) ===\n",
      "Client result (Gemini):\n",
      " Your loan application has been approved!\n",
      "\n",
      "The primary factors that strongly contributed to this approval are:\n",
      "*   **feat_3**: This feature had the most significant positive impact on your approval.\n",
      "*   **feat_2**: This feature also played a substantial role in the positive decision.\n",
      "*   **feat_7**: This feature further supported the approval of your loan.\n",
      "\n",
      "While `feat_5` and `feat_9` had a minor negative influence, the overall strength of your application, driven by the positive factors, led to the approval.\n",
      "Regulator result (Gemini):\n",
      " For the provided customer, the model's prediction is primarily influenced by `feat_3` and `feat_2`, which have the largest positive contributions, followed by `feat_7`. Conversely, `feat_5` and `feat_9` have negative contributions, meaning they decrease the likelihood of the predicted outcome. This explanation provides transparency into the key factors driving the decision for this specific customer.\n",
      "Executive result (Gemini):\n",
      " **Loan Approval Decision for Customer:**\n",
      "\n",
      "The customer's loan application has been **approved** with a high probability of **98.7%**.\n",
      "\n",
      "**Key Factors Influencing Approval:**\n",
      "\n",
      "The most significant positive contributions to this approval came from:\n",
      "*   **feat_3**\n",
      "*   **feat_2**\n",
      "*   **feat_7**\n",
      "\n",
      "Conversely, **feat_5** and **feat_9** had negative contributions, slightly reducing the probability of approval, but not enough to change the overall positive decision.\n",
      "\n",
      "This analysis provides a clear understanding of why this specific customer's loan was approved.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4) Demo runs (DSPy+Gemini)\n",
    "# -----------------------------------------------------------\n",
    "sample_idx = X_test.index[0]\n",
    "sample_row = X_test.loc[sample_idx].to_dict()\n",
    "print(\"\\n--- Running demo runs ---\\n\")\n",
    "\n",
    "print(\"=== Invoking DSPy + Gemini agent (real LLM) ===\")\n",
    "\n",
    "# Example calls\n",
    "client_result = dsp_agent(role=\"Client\", customer_data=sample_row)\n",
    "print(\"Client result (Gemini):\\n\", client_result)\n",
    "with open(OUT_DIR / \"client_output_gemini.txt\", \"w\") as f:\n",
    "    f.write(str(client_result))\n",
    "\n",
    "regulator_result = dsp_agent(role=\"Regulator\", customer_data=sample_row)\n",
    "print(\"Regulator result (Gemini):\\n\", regulator_result)\n",
    "with open(OUT_DIR / \"regulator_output_gemini.txt\", \"w\") as f:\n",
    "    f.write(str(regulator_result))\n",
    "\n",
    "executive_result = dsp_agent(role=\"Executive\", customer_data=sample_row)\n",
    "print(\"Executive result (Gemini):\\n\", executive_result)\n",
    "with open(OUT_DIR / \"exec_output_gemini.txt\", \"w\") as f:\n",
    "    f.write(str(executive_result))\n",
    "\n",
    "# If DSPy provides a tool-call trace API, fetch it here and save; otherwise rely on TOOL_LOG.\n",
    "with open(OUT_DIR / \"tool_log.json\", \"w\") as f:\n",
    "    json.dump(TOOL_LOG, f, indent=2)\n",
    "\n",
    "\n",
    "# End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_exp_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
